- day: 30
  month: Jan
  title: "RedHat OpenShift 4.2 ‚Äì Installations Pitfalls, Networking und Storage"
  subtitle: |
            <img src="/images/blogposts/OpenShift 4.2.png" alt="OpenShift 4.2"/>
            <a href="https://www.puzzle.ch/de/home">Puzzle ITC</a>, einer der Gr√ºnderfirmen von APPUiO, hat eine Woche genutzt, um ihr Wissen rund um 4.2 zu vertiefen. In der sogenannten  <a href="https://www.puzzle.ch/de/blog/articles/2020/01/15/mid-week">/mid Week</a> erzielten die Members spannende Erkenntnisse bei der Installation, Networking und Storage. r </a>.
  details: |
             <p>Der Blog ist aufgeteilt in drei Themen:</p>
             <li><b>Installations-Pitfalls:</b> In der /mid Week haben wir OpenShift 4.2 sowohl auf einem VMware vSphere Cluster wie auch bei AWS, GCP und Azure installiert. Wir berichten √ºber die Probleme, die dabei aufgetaucht sind.</li>
             <li><b>Networking:</b> Je nach Cloud Plattform gibt es logischerweise Unterschiede (z.B. Load Balancer). Wir haben aber auch Unterschiede zwischen OpenShift 3.11 und 4.2 festgestellt.</li>
             <li><b>Storage:</b> Jeder Cloud Provider bietet diverse Typen von Storage zur Integration in OpenShift an. Wir zeigen euch, was es f√ºr M√∂glichkeiten gibt.</li>
             <h4>Installation Pitfalls</h4>
             <img src="/images/blogposts/Pitfalls.jpg" alt="Pitfalls"/>
             <p>Wenn du in diesem Blog nach einer Anleitung suchst, wie OpenShift 4.2 installiert wird, bist du leider falsch. F√ºr die Installationen haben auch wir die offiziellen <a href="https://docs.openshift.com/container-platform/4.2/welcome/index.html">Anleitungen von RedHat</a> verwendet.</p>
             <p>Vorneweg: Die Installation wird mit OpenShift Version 4.2 deutlich einfacher und schneller als mit 3.11. Generell - egal f√ºr welche Zielplattform - haben wir Folgendes festgestellt:</p>
             <li>Kleinere Umgebungen als 3 Master / 3 Node (Standardvorgabe) sollten nicht gew√§hlt werden. Es wird sehr langsam oder die Installation schl√§gt teilweise sogar fehl. </li>
             <li>F√ºr den OpenShift Installer wird ein <em>install-config.yaml</em> File erstellt. Dies sollte vor Beginn der Installation gesichert werden, da der Installer dieses anschliessend l√∂scht.</li>
             <li>Weiter sollten auch alle Terraform Output Files gesichert werden, damit sp√§ter der Cluster einfach gel√∂scht werden kann. Achtung, beim L√∂schen des Cluster erfolgt keine zus√§tzliche Best√§tigung!</li>
             <li>Je nach Plattform haben wir unterschiedliche Gr√∂ssen der Standard-VM festgestellt.</li>
             <li>Wenn <a href="https://docs.openshift.com/container-platform/4.2/installing/installing_aws/installing-aws-customizations.html#ssh-agent-using_install-customizations-cloud">w√§hrend der Installation</a> kein SSH-Keyfile angegeben wurde, kann anschliessend nicht auf die VM's per SSH zugegriffen werden.</li>
             <p>Die Installation eines Clusters dauert je nach Plattform unterschiedlich lang:</p>
             <img src="/images/blogposts/Installationsdauer.png" alt="Installationsdauer"/>
             <h4>VMware vSphere</h4>
             <p>Einige Bemerkungen zur VMware vSphere Installation:</p>
             <li>Die Dokumentation zur Installation war sehr gut und wir konnten dieser Schritt f√ºr Schritt folgen.</li>
             <li>W√§hrend der (ersten) Installation mussten wir feststellen, dass Reverse DNS Eintr√§ge zwingend notwendig sind. Die Installation war blockiert und wir mussten von neuem beginnen.</li>
             <li>Kleine Fehler in den Ignition (JSON) Files f√ºhren zu Fehler, die leider sehr schwer zu finden sind, da keine sinnvolle Fehlermeldung vorhanden ist. So hat uns z.B. ein fehlendes Komma etwa eine Stunde Zeit gekostet. Ignition Files k√∂nnen <a href="https://coreos.com/validate/">hier</a> validiert werden.</li>
             <li>F√ºr die Installation muss ein Load Balancer (z.B. HAproxy) erstellt werden. Siehe auch unten im Teil zu Networking.</li>
             <li>Infrastructur MachineSets f√ºr vSphere sind noch nicht implementiert. Daher ist die Installation auf VMware vSphere auch eine <a href="https://blog.openshift.com/openshift-4-install-experience/">User Provisioned Infrastructure (UPI)</a> Installation.</li>
             <p><b>GCP</b></p>
             <p>Damit die Installation auf GCP funktioniert, m√ºssen die folgenden APIs aktiviert sein:</p>
             <li>Identity and Access Management(IAM)</li>
             <li>Cloud Resource Manager API</li>
             <p>Weiter muss die Disksize Limit von 500GB auf 750 GB erh&ouml;ht werden (640GB benutzt nach der Installation). Das Definieren im <em>install-config.yaml</em> File des OpenShift Installer funktioniert leider nicht:</p>
             <pre class="western">
             platform:
                gcp:
                   rootVolume:
                   iops: 4000
                   size: 50
                   type: io1
                type: n1-standard-4</pre>
             <p><b>Azure</b></p>
             <p>Einige Bemerkungen zur Installation auf Azure:</p>
             <li>Free Tier Subscription reicht nicht aus f√ºr eine OpenShift Installation.</li>
             <li>Es m√ºssen Anpassungen an den default Azure Account Limits gemacht werden.</li>
             <li>Z√ºrich befindet sich z.Z. nicht unter den supported Azure Regions.</li>
             <li>Die Dokumentation ist falsch bzgl. <em>Azure account limits & Creating a service principal</em>.</li>
             <p><b>AWS</b></p>
             <p>Die Installation auf AWS war am einfachsten. Das liegt wohl daran, dass bereits Openshift 4.0 darauf ausgelegt war.</p>
             <h4>Networking</h4>
             <img src="/images/blogposts/networking.png" alt="networking"/>
             <p><b>Load Balancing</b></p>
             <p>F√ºr eine RedHat OpenShift 4.2 Installation werden zwei Load Balancer vor dem Cluster ben√∂tigt:</p>
             <li><b>API Load Balancer:</b> F√ºr eine Hochverf√ºgbarkeit der Kubernetes-API (welche auf dem Master l√§uft), m√ºssen alle API Calls an diese Master Nodes verteilt werden.</li>
             <p>Auf den drei Cloud Provider konnten wir daf√ºr jeweils den Load Balancer Service des Providers verwenden. Dieser wird mit dem OpenShift Installer automatisch konfiguriert. Auf VMware vSphere mussten wir daf√ºr selber einen Load Balancer konfigurieren. Wir haben daf√ºr eine CentOS VM mit HAProxy verwendet. F√ºr die hohe Verf√ºgbarkeit kann z.B. keepalived verwendet werden.</p>
             <li><b>Client Access Load Balancer:</b> F√ºr den Zugriff auf den Applikation Workload wird ein Load Balancer ben√∂tigt, der die Ingress Controller weiterleitet.</li>
             <p>F√ºr den Client Access Load Balancer kann auf die Cloud Provider Integration von Kubernetes zur√ºckgegriffen werden. Damit werden mit Hilfe eines <a href="https://kubernetes.io/docs/concepts/services-networking/service/#
             lancer">Kubernetes Services vom Typ Load Balancer</a> automatisch ein Load Balancer auf der entsprechenden Cloud Plattform provisioniert.</p>
             <p>Bei der on-premise Installation mit VMware vSphere muss der Load Balancer selber implementiert werden, welcher den Netzwork Traffic auf die Ingress Controller weiterleitet. Das automatische Erstellen des Load Balancers via Kubernetes Service funktioniert hier leider nicht.</p>
             <h4>Egress Traffic & NetworkPolicy</h4>
             <li><b>NetworkPolicy:</b>Die Kubernetes v1 NetworkPolicy Features sind in OpenShift 4.2 verf√ºgbar</li>
             <li><b>Egress IP:</b> Identisch zu OpenShift 3.11. <a href="https://docs.openshift.com/container-platform/4.2/networking/openshift-sdn/assigning-egress-ips.html">(Referenz)</a></li>
             <li><b>EngressNetworkPolicy</b> wir auf OpenShift 4.2 nicht unterst√ºtzt </li>
             <li><b>EgressRouter</b> wird auf OpenShift 4.2 nicht unterst√ºtzt </li>
             <h4>Storage</h4>
             <img src="/images/blogposts/storage.jpg" alt="storage"/>
             <p>Abschliessend schauen wir uns noch die diversen Storage Integrationen f√ºr OpenShift 4.2 an. Wir teilen Storage in drei Kategorien ein: Block Storage, File Storage und Object Storage.</p>
             <p><b>Black Storage</b></p>
             <p>Erfreulicherweise hatten wir mit keinem Provider (onpremise wie auch Cloud) Probleme. Nach der Installation gem√§ss Anleitung konnten wir Block Storage von allen Cloud Providern beziehen. Alle Infos dazu sind in der entsprechenden Dokumentation von RedHat zu finden.</p>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-gce.html">GCP</a></li>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-azure.html">Azure</a></li>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-aws.html">AWS</a></li>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-vsphere.html">vSphere</a></li>
             <p><b>File Storage</b></p>
             <p>Als File Storage bezeichnen wir solchen, der insbesondere shared bezogen werden kann <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">(ReadWriteMany - RWX)</a>.</p>
             <p><b>AWS</b></p>
             <p>Auf AWS steht uns <a href="https://aws.amazon.com/efs/">EWS</a> zur Verf√ºgung, dabei haben wir jedoch Folgendes festgestellt:</p>
             <li>Auf EFS sind alle Volumes nur Subfolder des Root Volume.</li>
             <li>Quotas k√∂nnen nicht forciert werden.</li>
             <li>Keine Usage Metrics</li>
             <li><em>size</em> in einem PVC wird nicht ber√ºcksichtigt.</li>
             <li>RedHat sagt zu EFS: <em>"Elastic File System is a Technology Preview feature only.[...]"</em> <a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-efs.html">Openshift doc on EFS</a></li>
             <li>upstream efs-provisioner: <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs">detailed doc & code</a></li>
             <p>Weiter kann auch Netapp Trident verwendet werden. In unserer /mid Week haben wir dies jedoch nicht angeschaut (auch nicht auf den anderen Cloud Provider Plattformen). Daf√ºr wird ein AWS Account mit konfiguriertem NetApp CVS (1TB / 100$ / Monat) ben√∂tigt. Infos dazu in der <a href="https://netapp-trident.readthedocs.io/en/stable-v19.07/kubernetes/operations/tasks/backends/cvs_aws.html">Netapp Trident Dokumentation</a>.</p>
             <p><b>Azure</b></p>
             <p>Microsoft bietet mit Azure File die M√∂glichkeit, dynamisch File Storage zu beziehen. Die folgenden Features werden dabei aber nicht supported.</p>
             <li>Symlinks</li>
             <li>Hard links</li>
             <li>Extended attributes</li>
             <li>Sparse files</li>
             <li>Named pipes</li>
             <p>Alle Informationen dazu sind in der <a href="https://docs.openshift.com/container-platform/4.2/storage/dynamic-provisioning.html#azure-disk-definition_dynamic-provisioning">RedHat OpenShift Dokumentation</a> zu finden. Auch auf Azure kann <a href="https://netapp-trident.readthedocs.io/en/stable-v19.07/kubernetes/operations/tasks/backends/anf.html">Netapp Trident</a> verwendet werden.</p>
             <p><b>GCP</b></p>
             <p>Auf GCP stehen <a href="https://cloud.google.com/solutions/filers-on-compute-engine">mehrere M√∂glichkeite</a>n zur Verf√ºgung.</p>
             <p>F√ºr <a href="https://cloud.google.com/community/tutorials/gke-filestore-dynamic-provisioning">Cloud File Store kann der nfs-client-provisioner</a> verwendet weren. Auch hier k√∂nnen Quotas nicht forciert werden. Weiter gibt es einen nicht offiziell supporteten <a href="https://github.com/kubernetes-sigs/gcp-filestore-csi-driver">CSI Treiber</a>, den wir aber nicht wirklich zum Laufen gebracht haben.</p>
             <p>Weiter kann auch hier NetApp Cloud Volumes mit Trident verwendet werden. Wir haben hierzu aber keine Dokumentation gefunden. Dies sollte aber √§hnlich wie bei <a href="https://netapp-trident.readthedocs.io/en/stable-v19.04/kubernetes/operations/tasks/backends/cvs_aws.html">AWS</a> seine. Weitere Infos dazu sind <a href="https://cloud.google.com/solutions/filers-on-compute-engine#netapp">hier</a> zu finden.</p>
             <p>Eine weitere M√∂glichkeit ist die Verwendung von <a href="https://github.com/Elastifile/elastifile-provisioner">Elastifile</a> oder <a href="https://github.com/quobyte/quobyte-csi">Quoabyte</a>. Diese m√ºssen aber alle lizenziert werden. Elastifile ist nur f√ºr einen eingeschr√§nkten Kundenkreis verf√ºgbar. Quoabyte sieht zur Zeit noch nicht wirklich Enterprise-like aus.</p>
             <p><b>VMware vSphere</b></p>
             <p>In einer on-premise VMware vSphere Umgebung muss einen File Storage Service selbst aufgebaut werden (z.B. mit <a href="https://www.openshift.com/products/container-storage/">RedHat Contrainer Storage</a> basierend auf <a href="https://rook.io/">rook.io</a>).</p>
             <p><b>Object Storage</b></p>
             <p>Auch Object Storage wird von den drei Cloud Provider angeboten. Dieser wird aber in der Regeln nicht als PV in einen Pod gemounted, sondern direkt aus einer Applikation bezogen. Der Vollst√§ndigkeit halber hier eine Auflistung der Object Storage Services.</p>
             <li>GCP mit <a href="https://cloud.google.com/storage/">Google Cloud Storage</a></li>
             <li>AWS mit <a href="https://aws.amazon.com/s3/">S3</a></li>
             <li>Azure mit <a href="https://azure.microsoft.com/en-us/services/storage/">Azure Object Storage</a></li>
             <p>In einer on-premise VMware vSphere Umgebung muss ein Object Storage Service selbst aufgebaut werden.</p>

- day: 09
  month: Dez
  title: "Quarkus on APPUiO"
  subtitle: |
            <img src="/images/blogposts/Quarkus_on_APPUiO.png" alt="Quarkus on APPUiO"/>
            With the release of Quarkus 1.0.0, Java has once again become attractive for microservices. In this blog post I show you how to run native applications on APPUiO using Quarkus. This is a guest article by <a href="https://nxt.engineering/en/team/mig/">Michael Gerber</a> form <a href="https://nxt.engineering/en/">nxt Engineering GmbH</a>.
  details: |
            <h4>Create a Quarkus Project</h4>
            <p>The easiest starting point to create a <a href="https://quarkus.io/">Quarkus</a> project is the website <a href="https://code.quarkus.io">https://code.quarkus.io</a>. The same concept is already known from Spring Boot <a href="https://start.spring.io/">https://start.spring.io</a>. This makes it easy to create a project with the needed libraries.</p>
            <img src="/images/blogposts/Quarkus_APPUiO_1.png" alt="Quarkus APPUiO 1"/>
            <p>For this example choose the extensions <em>RESTEasy JAX-RS</em> and <em>SmallRye Health</em>. RESTEasy JAX-RS, which is included as default, is used for REST web services and SmallRye Health is an extension that helps to integrate health checks.</p>
            <h4>Start the Application</h4>
            <p>To start Quarkus locally you just need to have Java 8 installed. With the command <em>./mvnw compile quarkus:dev</em> you can start the application in development mode. Changes in the Java code are applied directly in the running application via hot deployment. Thus, a time-consuming application restart is not necessary üòÉ.</p>
            <h4>Create a REST Web Service</h4>
            <p>As an example application, we develop a REST web service that calculates a specific number in the <a href="https://en.wikipedia.org/wiki/Fibonacci_number">Fibonacci sequence</a>. The sequence is named after <a href="https://en.wikipedia.org/wiki/Fibonacci">Leonardo Fibonacci</a>, who described the growth of a rabbit population in 1202. The Fibonacci sequence can be calculated recursively as well as iteratively. To push the CPU a little bit, I use the recursive variant.</p>
            <pre><code>package nxt;
            
            import javax.ws.rs.GET;
            import javax.ws.rs.Path;
            import javax.ws.rs.PathParam;
            import javax.ws.rs.Produces;
            import javax.ws.rs.core.MediaType;
            
            @Path("/fibonacci")
            public class FibonacciResource {
            
                @GET
                @Produces(MediaType.TEXT_PLAIN)
                @Path("{number}")
                public int fibonacci(@PathParam("number") Integer number) {
                    if (number == 0) {
                            return 0;
                    }
                    if (number == 1) {
                            return 1;
                    }
                    return fibonacci(number - 2) + fibonacci(number - 1);
                }
            }
            </code></pre>
            <p>The application can be tested with curl as follows:</p>
            <pre><code>curl http://0.0.0.0:8080/fibonacci/10</code></pre>
            <h4>Test a REST Web Service</h4>
            <p>Quarkus uses the well-known library <a href="http://rest-assured.io/">REST-assured</a> for REST web service tests. The web service created earlier can be tested with the following code:</p>
            <pre><code>package nxt;
            
            import io.quarkus.test.junit.QuarkusTest;
            import org.junit.jupiter.api.Test;
            
            import static io.restassured.RestAssured.given;
            import static org.hamcrest.CoreMatchers.is;
            
            @QuarkusTest
            public class FibonacciResourceTest {
            
                @Test
                public void testEndpoint() {
                   testFibonacci(0, 0);
                   testFibonacci(1, 1);
                   testFibonacci(2, 1);
                   testFibonacci(3, 2);
                   testFibonacci(4, 3);
                   testFibonacci(5, 5);
                   testFibonacci(6, 8);
                }
                private void testFibonacci(int number, int fibonacci) {
                   given()
                      .when().get("/fibonacci/"+ number)
                      .then()
                        .statusCode(200)
                        .body(is(String.valueOf(fibonacci)));
                }
            }
            </code></pre>
            <p>The test can be executed with the command <em>./mvnw test.</em></p>
            <h4>Build Application on APPUiO</h4>
            <p>For the application to run on APPUiO, a Swiss OpenShift platform, it must first be built. The Quarkus team provides an OpenShift Source-to-Image (S2I) build.</p>
            <p>With the following command you can create a new build on OpenShift and start it automatically:</p>
            <pre><code>oc new-build quay.io/quarkus/ubi-quarkus-native-s2i:19.2.1~https://gitlab.com/nxt/public/quarkus-fibonacci.git \
              --name=quarkus-fibonacci-build
            </code></pre>
            <p>The <a href="https://www.graalvm.org/">GraalVM</a>, which creates a native image from the Java code, needs a lot of computing power. In order for the build on APPUiO to acquire the needed resources, it has to be configured appropriately.</p>
            <pre><code>oc patch bc/quarkus-fibonacci-build \
              -p '{"spec":{"resources":{"requests":{"cpu":"0.5", "memory":"2Gi"},"limits":{"cpu":"4", "memory":"4Gi"}}}}'
            </code></pre>
            <p>The above command sets the build job limit to 4 CPUs and 4 gigabytes of RAM.</p>
            <p>The great thing about APPUiO is that the resources needed for the build are not pulled from the project‚Äôs own resources. So you can push your own project to the limit and still run builds on OpenShift üòé.</p>
            <p>The disadvantage of this approach is that the resulting docker image has a size of 600 MB ü§î. This is because the docker image contains the entire <em>GraalVM</em>. A Docker multistage build can solve this problem nicely.</p>
            <p>The following docker file contains a multistage build where the native image is built first with <em>GraalVM</em> and after that a minimal docker image based on <em>ubi-minimal is built</em>.</p>
            <pre><code>## Stage 1 : build with maven builder image with native capabilities
            FROM quay.io/quarkus/centos-quarkus-maven:19.2.1
            COPY src /usr/src/app/src
            COPY pom.xml /usr/src/app
            USER root
            RUN chown -R 1001 /usr/src/app
            USER 1001
            RUN mvn -f /usr/src/app/pom.xml package -Pnative -e -B -DskipTests -Dmaven.javadoc.skip=true -Dmaven.site.skip=true -Dmaven.source.skip=true -Djacoco.skip=true -Dcheckstyle.skip=true -Dfindbugs.skip=true -Dpmd.skip=true -Dfabric8.skip=true -Dquarkus.native.enable-server=true
            
            ## Stage 2 : create the docker final image
            FROM registry.access.redhat.com/ubi8/ubi-minimal
            WORKDIR /work/
            COPY --from=0 /usr/src/app/target/*-runner /work/application
            RUN chmod 775 /work
            EXPOSE 8080
            CMD ["./application", "-Dquarkus.http.host=0.0.0.0"]</code></pre>
            <p>On OpenShift, you can run multistage docker builds. The first of the following two commands creates the image stream for the new lean docker image and the second one creates the docker build.</p>
            <pre><code>oc create is quarkus-fibonacci
            oc create -f - | << EOF
            {
                "apiVersion": "build.openshift.io/v1",
                "kind": "BuildConfig",
                "metadata": {
                    "labels": {
                         "build": "quarkus-fibonacci-build"
                    },
                    "name": "quarkus-fibonacci-build"
                },
                "spec": {
                    "output": {
                        "to": {
                            "kind": "ImageStreamTag",
                            "name": "quarkus-fibonacci:latest"
                        }
                    },
                    "resources": {
                        "limits": {
                             "cpu": "4",
                             "memory": "4Gi"
                         },
                         "requests": {
                             "cpu": "500m",
                             "memory": "2Gi"
                         }
                    },
                    "source": {
                         "git": {
                             "uri": "https://gitlab.com/nxt/public/quarkus-fibonacci.git"
                         },
                         "type": "Git"
                    },
                    "strategy": {
                         "type": "Docker"
                    }
                }
            }
            EOF
            </code></pre>
            <h4>Publish Application to APPUiO</h4>
            <p>The previously built application can be published on APPUiO with the following two commands:</p>
            <pre><code>oc new-app --image-stream=quarkus-fibonacci:latest
            oc expose svc quarkus-fibonacci
            </code></pre>
            <p>The first command creates a deployment with a Pod and a corresponding service. The second command creates a route for the given service that makes the microservice available to the public.</p>
            <p>The command below allows you to test the newly deployed application:</p>
            <pre><code>curl http://$(oc get route | grep quarkus-fibonacci | awk '{print $2}')/fibonacci/1</code></pre>
            <h4>Set up Health Checks</h4>
            <p>Quarkus has an extension that offers health checks out of the box. If the extension has not yet been added to the project, it can be added with the command <em>./mvnw quarkus:add-extension -Dextensions="health"</em>. Quarkus will then automatically create health check endpoints that can be called via the URL <em>/health/live</em> and <b>/health/ready</b>.</p>
            <p>In OpenShift the health checks can be added with the following command:</p>
            <pre><code>oc set probe dc/quarkus-fibonacci --liveness --get-url=http://:8080/health/live --initial-delay-seconds=1
            oc set probe dc/quarkus-fibonacci --readiness --get-url=http://:8080/health/ready --initial-delay-seconds=1</code></pre>
            <h4>Autoscaling</h4>
            <p>Quarkus applications can be started extremely quickly. This feature is especially useful when using the autoscaling feature. This allows you to start and stop pod‚Äôs dynamically when needed.</p>
            <pre><code>oc autoscale dc/quarkus-fibonacci --min 1 --max 2 --cpu-percent=80</code></pre>
            <p>The command above adds autoscaling, which starts a second Pod when needed. As soon as the load decreases again, the additional Pod is automatically shut down again.</p>
            <p>Autoscaling can be tested with the ApacheBench tool from Apache.</p>
            <pre><code>ab -n 5000 -c2 http://$(oc get route | grep quarkus-fibonacci | awk '{print $2}')/fibonacci/30</code></pre>
            <p>This command sends 5000 parallel requests to the microservice. After a few seconds OpenShift will start the second Pod, which will be ready to use immediately.</p>
            <img src="/images/blogposts/quarkus_APPUiO_2.png" alt="Quarkus on APPUiO 3"/>
            <h4>Conclusion</h4>
            <p>With Quarkus you can quickly and easily build a microservice with Java, which meets all requirements to be operated efficiently in an OpenShift or Kubernetes.</p>
            <p>You can study <a href="https://gitlab.com/nxt/public/quarkus-fibonacci">the complete code on in GitLab repository</a>.</p>
            
            
            
            
            
- day: 17
  month: Sept
  title: "Red Hat Forum 2019"
  subtitle: |
            <img src="/images/blogposts/APPUiO_Red-Hat-Forum_2019.jpg" alt="Red Hat Forum 2019"/>
            Vor gut einer Woche fand das 8. Red Hat Forum in Z√ºrich Oerlikon statt. Auch dieses Jahr war APPUiO als Sponsor mit einem Stand vertreten. √úber 800 Teilnehmer warteten gespannt auf einen intensiven und spannenden Tag.
  details: |
            <h4>Red Hat und IBM</h4>
            <p>Die √úbernahme von Red Hat durch IBM war neben den Open Source-, DevOps-, Microservices- und Container-Themen ein zentraler Schwerpunkt. Nach Meinungen des Managements wie auch der einzelnen Mitarbeitern, sei IBM eine grosse Chance und ein ¬´Shareholder sowie Partner gleichzeitig¬ª. Es sei eine Gelegenheit, die neue T√ºren √∂ffnet. Von der Best√§ndigkeit Red Hat's wie sie heute ist, sind sie dennoch √ºberzeugt.</p>
            <img src="/images/blogposts/Red-Hat-Forum_Hut.jpg" alt="Red Hat Forum"/>
            <h4>APPUiO am Red Hat Forum</h4>
            <p>APPUiO war mit einem eigenen Stand vor Ort vertreten. Mit dabei nat√ºrlich: Ein gelber Container und das APPUiO-Team. F√ºr das Team stand der Austausch mit den Besuchern im Vordergrund. Nicht nur neue Kontakte konnten gekn√ºpft werden, hin und da besuchte ein bekanntes Gesicht den APPUiO-Stand. Spannende Keynotes und Breakout-Sessions liessen den tollen Tag abrunden.</p>
            <img src="/images/blogposts/APPUiO_Red-Hat-Forum_Retro.jpg" alt="APPUiO Retro"/>
            <p>Unser neuestes Gemeinschaftsprojekt. APPUiO on Philips. ;-) </p>
            <h4>APPUiO beerup</h4>
            <p>Was w√§re ein solch gelungener Tag bereits schon zu Ende? Das dachte sich APPUiO auch. Deshalb lud APPUiO die Besucher zu einem beerup in die Giesserei ein. Der Besucherdrang war noch gr√∂sser als beim ersten beerup im Sihlcity ‚Äì ein richtiger Erfolg. In ungezwungener Atmosph√§re wurde ein Bierchen getrunken, √ºber dies und jenes gesprochen und den Tag ausgeklungen.</p>
            <img src="/images/blogposts/APPUiO_beerup.jpg" alt="APPUiO beerup"/>
            <p>Aber nur wer in Besitz eines Golden-Tickets war, wurde von der strengen Einlasskontrolle hereingelassen.:-)</p>
            <img src="/images/blogposts/goldenticket.jpg" alt="APPUiO goldenticket"/>
            <p>Wir freuen uns schon auf das n√§chste Treffen, bis dahin: Macht's gut!</p>
            <p>Euer APPUiO-Team</p>

- day: 29
  month: Aug
  title: "OpenShift 4"
  subtitle: |
            <img src="/images/blogposts/OpenShift4.png" alt="OpenShift4"/>
            Seit Juni dieses Jahres ist OpenShift 4.1 verf√ºgbar, der erste √∂ffentlich zug√§ngliche Release von Red Hat (Version 4.0 war ein rein interner Release).
            Wir m√∂chten dir mit einer Blogpost-Serie Informationen, Erfahrungsberichte, Empfehlungen sowie Tipps und Tricks weitergeben, damit du fr√ºhzeitig √ºber die n√∂tigen Informationen verf√ºgst. Zus√§tzlich werden wir verschiedene Events wie beerups oder Techtalks organisieren, damit du detailliertere und technischere Berichte erh√§ltst.
            Falls du in deinem Unternehmen Unterst√ºtzung ben√∂tigst oder wir dir m√∂gliche Wege zu OpenShift 4 aufzeigen sollen, darfst du dich gerne bei uns melden.
            Starten wir mit ein paar Grundlagen zu OpenShift 4.
  details: |
            <h4>Entwickler-Tools</h4>
            <p>Mit OpenShift 4 √§ndert sich viel und doch nicht, zumindest aus Entwicklersicht. Die Verwendung von OpenShift 4 wird nichts bis fast nichts √§ndern. Um das Leben eines Entwicklers zu vereinfachen, hat Red Hat ein Tool <code> odo </code> entwickelt, welches nur die f√ºr Entwickler relevanten <code> oc-</code>Befehle enthalten wird. Ausserdem wird mit dem <a href="https://developers.redhat.com/products/codeready-workspaces/overview">Red Hat CodeReady Workspaces</a> eine "Kubernetes-native developer workspace server and IDE" zur Verf√ºgung gestellt, um das Entwickeln von auf OpenShift lauff√§higen Applikationen zu vereinfachen.</p>
            <h4>Installation</h4>
            <p>Die Installation von OpenShift 4 wurde stark vereinfacht. Mit dem einfachen Befehl <code>openshift-install create cluster</code> kann der Installationsassistent gestartet werden. Ohne weiteres Zutun fragt dieser die ben√∂tigten Konfigurationsparameter ab, die er nicht selbst herausfinden kann. F√ºr alles andere werden vern√ºnftige Defaults verwendet und so automatisch die Referenzarchitektur eingehalten.</p>
            <p>Die Control Plane Hosts werden immer mit Red Hat CoreOS (RHCOS) aufgesetzt, bei den restlichen besteht die Wahl zwischen RHCOS und klassischem RHEL. Der Vorteil der Verwendung von RHCOS besteht darin, dass OpenShift das Betriebssystem selbst verwalten und somit auch automatisch aktualisieren kann.</p>
            <p>Auf unterst√ºtzten Plattformen ist der Installer in der Lage, die gesamte zugrundeliegende Infrastruktur selbst zu provisionieren. Diese Art von Installation wird mit IPI (Installer Provisioned Infrastructure) bezeichnet und stellt die empfohlene Installationsvariante dar. Bei der anderen Art von Installation, UPI (User Provisioned Infrastructure), wird die Infrastruktur, wie es der Name bereits andeutet, selbst aufgebaut und dem Installer zur Verf√ºgung gestellt.</p>
            <p>Updates k√∂nnen neu √ºber die Web Console durchgef√ºhrt werden, wobei zwischen drei Channels (stable, pre-release, nightly) ausgew√§hlt werden kann.</p>
            <p>Schaut man unter die Haube von OpenShift 4, f√§llt auf, dass nebst der Kernkomponente Kubernetes die Container Engine ausgewechselt wurde. Anstelle von Docker kommt neu <a href="https://cri-o.io/">CRI-O</a> zum Einsatz. CRI-O verwendet als darunter liegende Container Runtime <code>runc</code>, wie dies auch Docker tut, und ist komplett <a href="https://www.opencontainers.org/">OCI</a>-compliant. Beispielsweise k√∂nnen mit Docker gebaute Images auch mit CRI-O problemlos gestartet werden - also kein Grund zur Sorge in dieser Hinsicht. Einer der Gr√ºnde f√ºr diesen Wechsel war, den monolithischen Docker-Daemon in einzelne Tools mit jeweils einem bestimmten Zweck aufzuteilen, ganz gem√§ss der Unix-Philosophie. So wurden nebst CRI-O die Container Tools <a href="https://buildah.io/">buildah</a>, <a href="https://podman.io/">Podman</a> und <a href="https://github.com/containers/skopeo">skopeo</a> ins Leben gerufen und sind schon seit einiger Zeit verf√ºgbar.</p>
            <h4>Update von OpenShift 3 auf 4</h4>
            <p>Das Wichtigste vorweg: Es wird kein Update-Pfad von OpenShift 3 auf 4 geben. Red Hat stellt aber ein Migrations-Tool zur Verf√ºgung, welches nicht nur die Kubernetes Ressourcen, sondern sogar die Daten von Persistent Volumes migrieren kann. Dabei wird S3 Storage als Zwischenspeicher verwendet. Das Migrations-Tool unterst√ºtzt neben Migrationen von Version 3 auf 4 auch Migrationen zwischen unterschiedlichen v4 Clustern.</p>
            <h4>Operators</h4>
            <p>Dass Operators ein wesentlicher Bestandteil von OpenShift 4 sein werden, ist bereits weitherum bekannt. Was Operators aber genau sind, wohl noch weniger: Ein Operator ist eine Methode f√ºr die Paketierung, das Deployment sowie die Verwaltung von Kubernetes-nativen Applikationen. Eine Kubernetes-native Applikation ist eine Applikation, welche sowohl auf Kubernetes deployt wie auch √ºber die Kubernetes API verwaltet werden kann.</p>
            <p>Ein Operator ist grunds√§tzlich ein Custom Controller, wobei der Controller zu den Kernkonzepten von Kubernetes geh√∂rt. Er vergleicht regelm√§ssig den gew√ºnschten mit dem effektiven Zustand einer oder mehrerer Ressourcen auf dem Cluster und korrigiert diesen falls n√∂tig. √Ñhnlich wie dies auch Puppet tut. Ein Operator selbst l√§uft als Pod auf dem Cluster.</p>
            <p>Operators √ºbernehmen in OpenShift 4 eine zentrale Rolle. Sie sind f√ºr die Steuerung und √úberwachung von so ziemlich jeder einzelnen Komponente verantwortlich, darunter auch kritische Netzwerk- und Credential-Dienste. Wiederum ein Operator √ºbernimmt die Verwaltung aller dieser Operators, der sog. Cluster Version Operator. Nebst diesen vom Cluster Version Operator verwalteten Plattform-Operators k√∂nnen auch Applikationen Gebrauch vom Operator Framework machen. Sie werden allerdings nicht vom Cluster Version Operator, sondern vom Operator Lifecycle Manager (OLM) verwaltet. Eine √úbersicht verf√ºgbarer Operators ist auf <a href="https://operatorhub.io/">OperatorHub.io</a> ersichtlich. Analog zu bspw. dem in Rancher integrierten App Catalog f√ºr 
            Charts, ist auch der OperatorHub in OpenShift 4 integriert und erm√∂glicht eine einfache, grafische Installation √ºber die Web Console.</p>
            <h4>Operator Framework</h4>
            <p>Operators k√∂nnen aber auch selbst geschrieben werden, bspw. mithilfe des Operator Frameworks. Das Framework unterst√ºtzt Helm, Ansible sowie Go, wobei jede Variante nat√ºrlich seine eigenen Vor- und Nachteile hat:</p>
            <li>Helm ist sehr einfach zu schreiben, da kein Code geschrieben werden muss. Zudem wird auch kein Tiller mehr ben√∂tigt.</li>
            <li>Ansible ist f√ºr Betreiber die erste Wahl, da meist bereits Ansible-Know How vorhanden ist.</li>
            <li>Go stellt zwar die vermutlich herausforderndste, daf√ºr, aufgrund seiner vollwertigen Programmiersprache, aber auch die m√§chtigste und flexibelste Variante dar.</li>
            <p>Der sog. Capability Level dieser Varianten wird in folgender Abbildung aufgezeigt. Der Capability Level veranschaulicht, welche Phasen unterst√ºtzt werden.</p>
            <img src="/images/blogposts/operator-capability-level-transparent-bg.png" alt="Capability level"/>
            <h4>OpenShift Container Storage</h4>
            <p>Red Hat beschreibt den OpenShift Container Storage, oder kurz RHOCS oder OCS, als "Add-On for OpenShift for running stateful apps". W√§hrend OCS unter OpenShift 3 noch aus Gluster und Heketi bestand, um dynamisch Persistent Volumes zu allozieren, soll diese Aufgabe die Kombination aus <a href="https://rook.io/">Rook</a>, <a href="https://ceph.io/">Ceph</a> und <a href="https://www.noobaa.io/">NooBaa</a> √ºbernehmen. Gr√ºnde f√ºr diesen Wechsel seien insbesondere das beachtliche Momentum hinter der Entwicklung von Rook sowie der aus Sicht Red Hat zunehmende Fokus auf Object Storage, welcher von NooBaa in Form einer S3-kompatiblen API abgedeckt wird. Wie auch schon bei Version 3 werden ein Independent sowie ein Converged Mode angeboten. OCS kann also als externer Storage Cluster oder aber direkt auf OpenShift selbst installiert werden. Anders als beim Gluster-Heketi-Stack soll neu der Storage via CSI (Container Storage Interface) angebunden werden k√∂nnen.</p>
            <p>Geplant ist, OpenShift Container Storage 4 mit OCP Version 4.2 zu ver√∂ffentlichen, welches wiederum in Q3 2019 geplant ist. OpenShift Container Storage 3.X soll noch bis Juni 2022, gleich wie OpenShift Container Platform 3.X, <a href="https://access.redhat.com/support/policy/updates/openshift/">supportet sein</a>. Wie bereits bei OpenShift 3 wird RHOCS auch auf v4 durch die OpenShift Storage Addon-Subscription abgedeckt. Wer also bereits im Besitz einer solchen ist, ist mit OpenShift 4 bereits abgedeckt.</p>
            <h4>Service Mesh</h4>
            <p>Das OpenShift Service Mesh besteht nicht nur, wie h√§ufig angenommen, aus <a href="https://istio.io/">Istio</a>, sondern zudem aus <a href="https://www.kiali.io/">Kiali</a>, <a href="https://www.jaegertracing.io/">Jaeger</a> sowie <a href="https://www.envoyproxy.io/">Envoy Proxy</a>. Das Service Mesh erm√∂glicht besseres Tracking und Management der Kommunikation zwischen Services und Pods, indem ein Envoy Proxy als Sidecar-Container in die Pods hinzugef√ºgt wird. Envoy stellt dabei die Data Plane innerhalb des Service Mesh dar. Die Control Plane (nicht zu verwechseln mit der OpenShift Control Plane) ist verantwortlich f√ºr die Verwaltung und Konfiguration der Envoy Proxies um den Traffic zu routen wie auch Policies anzuwenden. Diese Konstellation ergibt ein Netzwerk von Services mit Load Balancing, Service-zu-Service-Authentifizierung, Monitoring und mehr. Code-√Ñnderungen an der Applikation sind dabei keine bis nur wenige notwendig.</p>
            <h4>Pipelines</h4>
            <p>OpenShift Pipelines setzt auf <a href="https://tekton.dev/">Tekton</a>, welches vorher unter dem Namen Knative Build-Pipeline bekannt war. Die Idee dahinter ist, cloud-native CI/CD unter Kubernetes zur Verf√ºgung zu stellen. Dabei werden die verschiedenen f√ºr eine Pipeline ben√∂tigten Komponenten (wie auch die Pipeline selbst) als Custom Resources angelegt und k√∂nnen so via <code>kubectl</code>/<code>oc</code> administriert werden. Gem. Roadmap soll mit Version 4.2 ein Tech Preview und mit 4.3 die GA-Version erh√§ltlich sein.</p>
            <h4>Serverless</h4>
            <p>Auch "serverless" darf nat√ºrlich nicht fehlen, welches mit <a href="https://knative.dev">Knative</a> realisiert wird. Gem√§ss Roadmap soll mit Version 4.2 ein Tech Preview und mit 4.3 die GA-Version erh√§ltlich sein. Was FaaS bedeutet kannst du im <a href="https://appuio.ch/blog.html#2017-Okt-10">Blogpost</a> von Tobru nachlesen.</p>
            <h4>Schlusswort</h4>
            <p>Die aufgef√ºhrten Neuerungen sind nat√ºrlich nicht abschliessend, daher kann sich ein Blick in die <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.1/html-single/release_notes/index">Release Notes von OpenShift 4.1</a> oder in einen der vielen anderen Artikel, Blogposts etc. lohnen. Mit OpenShift 4.2 wird der wohl erste Release ver√∂ffentlicht, der die meistgefragten Features und Unterst√ºtzungen mitbringt. Ein <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.1/html-single/release_notes/index">kurzer Blick</a> lohnt sich also auf alle F√§lle. Wir von APPUiO werden weiter Erfahrung mit dem brandneuen OpenShift-Release, auch in Zusammenarbeit mit unseren Kunden, sammeln und in eine bestm√∂gliche Unterst√ºtzung umsetzen.</p>       

- day: 26
  month: Aug
  title: "10 wissenswerte Fakten zu Kubernetes und OpenShift"
  subtitle: |
            <img src="/images/blogposts/Kubernetes_OpenShift.png" alt="Kubernetes und OpenShift"/>
            Bei APPUiO setzen wir auf OpenShift von Red Hat. OpenShift ist eine Kubernetes Distribution. Kubernetes ist eine Plattform zur Orchestrierung von Container-Systemen. Alles klar? Bist du noch dabei? :-) Wenn ja, dann lies weiter und erfahre im folgenden Blogpost, was Kubernetes, eine Kubernetes Distribution und OpenShift ist und warum wir APPUiO auf den Grundlagen von Kubernetes und OpenShift aufgebaut haben.
  details: |
            <h4>1. Was ist Kubernetes?</h4>
            <p>Kubernetes ist die Plattform zur Orchestrierung von Container-Systemen. Kubernetes automatisiert das Einrichten, Betreiben und auch das Skalieren von containerisierten Anwendungen. Die Open Source Plattform wird auch mit "K8s" abgek√ºrzt und das Wort Kubernetes kann mit "Steuermann" √ºbersetzt werden.</p>
            <p>Zu den Funktionen von Kubernetes z√§hlen unter anderem die Automatisierung von Containern und des Software Rollouts, die Optimierung der eingesetzten Ressourcen, Persistent Storage, Service Discovery, Autoscaling und HA. Im Vergleich zu anderen Orchestrierungsplattformen, wie bspw. Docker Swarm, unterst√ºtzt Kubernetes auch andere containerbasierte Virtualisierungssysteme.</p>
            <p>Die offizielle Beschreibung von Kubernetes lautet:</p>
            <p><em>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.</em></p>
            <p>F√ºr das Verst√§ndnis dieses Beitrags ist es wichtig zu verstehen, dass es sich bei Kubernetes um eine Plattform handelt und nicht um ein fixfertiges Produkt ab der Stange.</p>
            <h4>2. Wer steckt hinter Kubernetes und wer entwickelt es weiter?</h4>
            <p>Kubernetes wurde urspr√ºnglich von Google entwickelt, um die ben√∂tigte riesige Infrastruktur f√ºr die Suchmaschine bereitzustellen und zu optimieren. Durch den Einsatz von Virtualisierung von Hardware, Cloud Computing, Site Reliability Engineering und Container-Technologien f√ºhrte die L√∂sung dazu, dass die bestehende Infrastruktur viel besser ausgelastet wurde. Dies f√ºhrte zu wesentlich geringeren Kosten der Infrastruktur-Landschaft. Zum Durchbruch verhalf Google Kubernetes aber dadurch, dass Kubernetes als Open Source L√∂sung zur Verf√ºgung gestellt und an die Cloud Native Computing Foundation (CNCF) gesoendet wurde.</p>
            <p>Heute z√§hlt Kubernetes zu den aktivsten Open Source Projekten weltweit und die <a href="https://kubernetes.io/partners/#kcsp">Partner-Landschaft</a> liest sich wie das Who-is-Who der IT-Welt. </p>
            <h4>3. Was ist eine Kubernetes Distribution?</h4>
            <p>Um die Unterschiede von Kubernetes und OpenShift zu verstehen, muss zuerst der Begriff "Kubernetes Distribution" gekl√§rt werden. Wird Kubernetes direkt aus dem Open Source Kubernetes Projekt installiert, erh√§lt man "nur" die Kernkomponenten (API Server, Controller Manager, Scheduler, Kubelet, kube-proxy). Damit Kubernetes aber auch wirklich nutzbar wird,  werden viele weitere Komponenten wie etcd, Ingress Controller, Logging Server, Metrics Collector (z.B. Prometheus), Software Defined Network (SDN) usw. ben√∂tigt. Dies ist gut vergleichbar mit Linux: Der Linux Kernel alleine bringt noch nicht viel. Es braucht eine ganze Linux Distribution, die eine Shell, das Paketmanagement, den Bootprozess und vieles mehr zur Verf√ºgung stellt.</p>
            <p><em>OpenShift ist eine Kubernetes Distribution und macht aus Kubernetes ein Produkt.</em></p>
            <p>Eine "Minimum Viable Kubernetes Distribution" ben√∂tigt folgende zus√§tzliche Komponenten und Tools f√ºr einen produktiven Betrieb:</p>
            <div style="padding-left: 30px;">‚ñ∏<b> Installations- und Upgrademechanismus:</b> F√ºr eine automatisierte Installation aller involvierten Komponenten.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> SDN (Software Defined Network):</b> Pods m√ºssen untereinander kommunizieren k√∂nnen, egal wo sie laufen. Dies stellt das SDN sicher.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Ingress Controller:</b> Damit der Benutzerzugriff auf die auf dem Cluster laufende Applikationen m√∂glich ist.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Authentication:</b> Eine zentrale Benutzer- und Gruppendatenbank stellt den authentisierten und autorisierten Zugriff zur Verf√ºgung.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Security:</b> Kubernetes f√ºhrt Container via Docker oder CRI-O aus. Die Sicherheit auf dem Hostsystem muss entsprechend gew√§hrleistet sein.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Persistent Storage:</b> Stateful Applikationen wie Datenbanken ben√∂tigen persistenten Storage.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Monitoring:</b> St√§ndige √úberwachung aller Clusterkomponenten und Applikationen.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Backup:</b> Sicherung der Clusterkomponenten und persistenten Daten.</div>
            <p>Optional werden weitere Komponente empfohlen:</p>
            <div style="padding-left: 30px;">‚ñ∏ Zentrales Logging mit grafischer Aufbereitung und Suchfunktion</div>
            <div style="padding-left: 30px;">‚ñ∏ Applikations- und Cluster Metrics inkl. Alerting</div>
            <h4>4. OpenShift als Kubernetes Distribution</h4>
            <p>Im Kern basiert OpenShift zu 100% auf Kubernetes, bringt aber als Kubernetes Distribution alles mit, was zur Benutzung eines Kubernetes Clusters ben√∂tigt wird. Um nur die wichtigsten Funktionen zu nennen:</p>
            <div style="padding-left: 30px;">‚ñ∏<b> Operations Tools:</b> Ein offizieller Weg via Ansible erm√∂glicht es, den gesamten Lifecycle von OpenShift durchzuf√ºhren. Dazu geh√∂rt die automatisierte Installation, wie auch Upgrades auf neuere Versionen von OpenShift. Mit OpenShift 4 beginnt eine neue √Ñra mit einem neuen Installations- und Operationsprozess, basierend auf Kubernetes Operators.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Router:</b> Der OpenShift Router (Ingress Controller) - basierend auf HAProxy - sorgt daf√ºr, dass der Zugriff auf Applikationen innerhalb des Clusters √ºber HTTP(S) erm√∂glicht wird.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Multi-Tenancy:</b> Die im Kern eingebaute Multi-Tenancy √ºber OpenShift Projekte, RBAC und weiteren Konzepten erm√∂glicht die Benutzung der Plattform durch verschiedene Tenants (Kunden).</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Authentication:</b> Es werden die unterschiedlichsten Authentication Backends unterst√ºtzt, allen voran LDAP, ActiveDirectory und viele mehr.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Metrics:</b> Die mitgelieferte Metrics Komponente sammelt alle verf√ºgbaren Messwerte (RAM, CPU, Netzwerk) der auf dem Cluster laufenden Applikationen und visualisiert diese in der Webkonsole.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Central Logging:</b> Alle von der Applikation auf¬†stdout¬†geloggten Zeilen werden automatisch von der zentralen Logging Komponente gesammelt und √ºber die Webkonsole dem Benutzer zur Verf√ºgung gestellt.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Security:</b> Die Plattform ist auf h√∂chste Sicherheit ausgelegt. So sorgen z.B. Sicherheitsmassnahmen im Kernel von Red Hat Enterprise Linux wie SELinux daf√ºr, dass die Sicherheit der Container gew√§hrleistet ist. Weitere Massnahmen wie "Security Context Constraints" (SCC) und das Verhindern von Root Containern sorgen f√ºr weitere Sicherheit.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Builds und Pipelines:</b> Direkt im Cluster integrierte Build- und Pipeline-Funktionalit√§ten erm√∂glichen einen komplett integrierten CI/CD Workflow.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Webkonsole:</b> Alle Vorg√§nge auf dem Cluster werden f√ºr den Anwender der Plattform in einer Webkonsole graphisch dargestellt und erm√∂glichen einen einfachen und schnellen Einstieg in die Benutzung von Kubernetes.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> SDN:</b> Das mitgelieferte Software Defined Networking sorgt f√ºr die Konnektivit√§t zwischen den auf der Plattform laufenden Pods und f√ºr eine angemessene Netzwerksicherheit mit Network Policies.</div>
            <div style="padding-left: 30px;">‚ñ∏<b> Container Registry:</b> Docker / Container Images werden in der mitgelieferten Registry gespeichert und zum Deployment auf die Worker Nodes benutzt.</div>
            <p>Alle diese von Haus aus mitgelieferten Funktionalit√§ten lassen sich zu jedem Kubernetes Cluster hinzuf√ºgen, was jedoch mit einem hohen Aufwand verbunden ist. Dies ist vergleichbar mit dem Bau einer eigenen Linux Distribution, wie z.B. das <a href="http://www.linuxfromscratch.org/">"Linux From Scratch"</a> veranschaulicht. F√ºr Kubernetes existiert eine √§hnliche Anleitung, genannt <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">"Kubernetes The Hard Way"</a>.</p>
            <h4>5. OpenShift als PaaS</h4>
            <p>Die St√§rke von Kubernetes liegt in der Container Orchestrierung. Zus√§tzlich dazu bietet OpenShift klassische Platform-as-a-Service (PaaS) Funktionen. Eine davon ist das automatische Builden und Deployen von Applikationscode direkt ab einem Git Repository. Trotzdem hat man als Anwender der Plattform dank der grossen Flexibilit√§t immer die Wahl, ob man die integrierten Buildfunktionen nutzen oder doch lieber ausserhalb des Clusters builden m√∂chte. Dies l√§sst sich f√ºr jedes Deployment entscheiden. So k√∂nnen auf einem Cluster beide Arten verwendet werden.</p>
            <h4>6. OpenShift als Upstream zu Kubernetes</h4>
            <p>Viele Entwicklungen in Kubernetes stammen urspr√ºnglich aus OpenShift. Als bestes Beispiel l√§sst sich RBAC (Role Based Access Control) nennen. Dieses Feature ist seit der ersten OpenShift-Version Bestandteil und wurde sukzessive in Kubernetes eingebaut. RBAC ist seit Kubernetes Version 1.6 fester Bestandteil von Kubernetes. Auch das OpenShift "Route"- oder das "DeploymentConfiguration"-Objekt hat die heutigen Objekte "Ingress" bzw. "Deployment" in Kubernetes massgeblich mitgepr√§gt.</p>
            <p>Da OpenShift zu 100% auf Kubernetes basiert, werden auch alle Kubernetes Native Workloads unterst√ºtzt, wie z.B. das "Deployment"- oder das "Ingress"-Objekt.</p>
            <p>Schaut man etwas genauer auf die <a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1">Contributor-Statistiken</a>, dann stellt man fest, dass Red Hat die Nummer 2 der Contributor-Firmen ist (nur Google ist noch weiter vorn). Somit ist Red Hat massgeblich an der Entwicklung von Kubernetes beteiligt. Mit dem Kauf der Firma CoreOS hat sich Red Hat geballtes Kubernetes Know-how angeeignet. Das Ergebnis der CoreOS Integration in Red Hat ist die Verschmelzung von OpenShift und Tectonic zu OpenShift Version 4.</p>
            <h4>7. Alternativen zu OpenShift</h4>
            <p>OpenShift ist nicht die einzige Kubernetes Distribution auf dem Markt. Ein kurzer Vergleich zeigt die Unterschiede:</p>
            <div style="padding-left: 30px;">‚ñ∏ <b> Cloud Vendor Kubernetes:</b>Die grossen Clouds bieten ihre eigenen Kubernetes Distributionen als Service an. Diese sind auf die jeweiligen Clouds zugeschnitten und werden von den Anbietern gepflegt. Eine Installation auf der eigenen Private Cloud oder auf anderen Public Clouds ist nicht m√∂glich.</div>
            <div style="padding-left: 50px;">‚ñ∏ <a href="https://aws.amazon.com/de/eks/">Amazon Elastic Container Service for Kubernetes (Amazon EKS)</a></div>
            <div style="padding-left: 50px;">‚ñ∏ <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a></div>
            <div style="padding-left: 50px;">‚ñ∏ <a href="https://azure.microsoft.com/de-de/services/kubernetes-service/">Azure Kubernetes Service (AKS)</a></div>
            <div style="padding-left: 50px;">‚ñ∏ <a href="https://www.ibm.com/cloud/container-service">IBM Cloud Kubernetes Service</a></div>
            <div style="padding-left: 50px;">‚ñ∏ <a href="https://www.alibabacloud.com/de/product/kubernetes"> Alibaba Cloud Container Service for Kubernetes</a></div>
            <div style="padding-left: 30px;">‚ñ∏ <b><a href="https://rancher.com/kubernetes/">Rancher:</a></b>Seit der Version 2.0 fokussiert sich Rancher zu 100% auf Kubernetes und bietet als grosse St√§rke eine Multi-Cluster Verwaltungsfunktion. So k√∂nnen mit Rancher Kubernetes Cluster in der Cloud (z.B. auf Amazon oder Google) zentral verwaltet werden, wie auch Kubernetes Cluster mit der "Rancher Kubernetes Engine" auf eigene VMs. Mit dem Webinterface gestaltet sich das Aufsetzen eines neuen Clusters sehr einfach und Applikationsdeployments mittels Helm sind auch direkt verf√ºgbar.</div>
            <div style="padding-left: 30px;">‚ñ∏ <b><a href="https://ubuntu.com/kubernetes">Canonical / Ubuntu Kubernetes</a></b> (Charmed Kubernetes): Plattform basierend auf Ubuntu, welches Juju als Installationstool verwendet. In Partnerschaft mit Google und Rancher wird in Zukunft eine Hybrid-Cloud-L√∂sung angeboten.</div>
            <div style="padding-left: 30px;">‚ñ∏ <b><a href="https://www.suse.com/de-de/products/caas-platform/">SUSE CaaS-Plattform:</a></b> Eine neue Plattform, basierend auf SUSE MicroOS. Mittels Salt wird die Konfigurationsverwaltung sichergestellt. Unter folgendem Link kann am Beta Programm teilgenommen werden:<a href="https://www.suse.com/betaprogram/caasp-beta/"> SUSE CaaS Platform Beta.</a></div>
            <p>Weitere Kubernetes Distributionen aufgelistet:</p>
            <div style="padding-left: 30px;">‚ñ∏ <b><a href="https://www.kontena.io/pharos/">Kontena Pharos</a></b></div>
            <div style="padding-left: 30px;">‚ñ∏ <b><a href="https://www.cisco.com/c/en/us/products/cloud-systems-management/container-platform/index.html/">Cisco Container Platform</a></b></div>
            <div style="padding-left: 30px;">‚ñ∏ <b><a href="https://cloud.netapp.com/kubernetes-service">NetApp Kubernetes Service</a></b></div>
            <div style="padding-left: 30px;">‚ñ∏ Und noch mehr aus der Kubernetes Dokumentation: <a href="https://kubernetes.io/docs/setup/">Getting started</a></div>
            <h4>8. Vendor Lock-ins</h4>
            <p>Ein sehr wichtiger zu beachtender Aspekt ist der Cloud- und/oder Vendor-Lock-In. Viele der Kubernetes Distributionen haben ihre eigene Eigenschaften, die unter Umst√§nden nicht miteinander kompatibel sind. Am Beispiel der "Cloud-Vendor"-Distributionen: Diese k√∂nnen nur in der entsprechenden Cloud benutzt werden. M√∂chte man jedoch einen Hybrid-Cloud-Ansatz verfolgen, ist dies durch den Lock-In nicht m√∂glich. Im Gegenzug erm√∂glicht eine selbst installierbare Distribution wie OpenShift diese Option.</p>
            <p>Reine Open Source Distributionen ohne Herstellersupport sind f√ºr produktive Umgebungen nicht zu empfehlen.</p>
            <h4>9. APPUiO - Swiss Container Platform</h4>
            <p>Dem aufmerksamen Leser ist bestimmt aufgefallen, dass zwischen der "Minimum Viable Kubernetes Distribution" und OpenShift gewisse Diskrepanzen bestehen. Genau dort setzt APPUiO an: Wir veredeln OpenShift zu einer vollst√§ndigen, production-ready Kubernetes Distribution, indem wir Managed Services anbieten. Wir √ºberwachen und sichern den Clusterstatus automatisch, k√ºmmern uns um regelm√§ssige Updates, beheben Fehler, stellen Persistent Storage zur Verf√ºgung und helfen mit unserem Know-how das Beste aus der Plattform herauszuholen. Durch unsere Erfahrung im Setup und Betrieb von OpenShift Clustern rund um die Welt, bieten wir Managed OpenShift Cluster auf nahezu jeder Public, Private oder On-Premise Cloud an. APPUiO hilft gerne bei der Evaluation, Integration und Betrieb und unterst√ºtzt mit ihrer langj√§hrigen Kubernetes und OpenShift Erfahrung.</p>
            <h4>10. Wo kann ich mehr erfahren?</h4>
            <p>Am <a href="https://www.meetup.com/de-DE/Cloud-Native-Computing-Switzerland/events/251026175/"> Cloud Native Meetup vom 28. August 2018</a> haben wir √ºbers Thema Kubernetes Distribution berichtet. Die Slides dazu sind auf <a href="https://speakerdeck.com/tobru/the-anatomy-of-a-kubernetes-distribution">Speaker Deck</a> zu finden. Ein empfehlenswerter Blogpost zum Thema Kubernetes Distributionen (auf Englisch) findet ihr hier: <a href="https://cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/">10 most important differences between OpenShift and Kubernetes</a>.</p>
            <p>Wenn du mehr √ºber OpenShift erfahren willst, besuche uns an unserem Stand am Red Hat Forum Z√ºrich am 10. September 2019 - 8:00 bis 18:00 im StageOne Z√ºrich-Oerlikon.</p>
            <p>Das gesamte APPUiO-Team freut sich auf deinen Besuch!</p>        

- day: 31
  month: Jul
  title: "Neu: OpenShift Dev Schulung"
  subtitle: |
            <img src="/images/blogposts/Fotoxy.png" alt="DevSchulung"/>
            Ab sofort bietet APPUiO OpenShift Dev Schulungen an. Wir zeigen dir, wie du deine Business Applikationen auf der OpenShift Plattform entwickeln, betreiben und monitoren kannst.
  details: |
            <p>Um unser langj√§hriges Wissen im Bereich der Software Entwicklung, Automatisierung und Betrieb von Cloud Native Applikationen auf OpenShift Container Plattformen weiterzugeben, haben wir die zweit√§gige OpenShift Dev Schulung erarbeitet. Sie baut das Know-How der Teilnehmer in einer guten Mischung aus Theorie und Hands-on Lab auf.</p>
            <p>Die Schulung beinhaltet folgende Themen, kann aber je nach Bed√ºrfnissen angepasst werden:</p>
            <div><b>Agenda ‚Äì day 1</b></div>
            <div>1. Introduction Linux Container / OpenShift Plattform</div>
            <div>2. Application Architecture / Cloud Native / 12 Factor Apps / Service Broker</div>
            <div>3. Introduction to develop an Application on OpenShift</div>
            <div>4. Hands-on: Build Strategies on OpenShift (s2i, binary, docker)</div>
            <div>5. Application Configuration / Service Discovery</div>
            <div>6. Hands-on: Build applications with different runtimes & deploy them on Openshift</div>
            <div>7. Share & Discuss exercise results / Wrapup</div>
            <br>
            <div><b>Agenda ‚Äì day 2</b></div>
            <div>1. Recap day 1</div>            
            <div>2. Troubleshooting & Debugging</div>            
            <div>3. Hands-on - Troubleshooting & Debugging</div> 
            <div>4. Backup / restore for Applications</div>
            <div>5. CI/CD Pipelines Introduction and Concepts</div>
            <div>6. Q&A for compamy specific usecases</div>
            <br>
            <div><b>Zus√§tzliche Themen auf Anfrage</b></div>
            <div>1. Coolstore Example (Microservice application showing different concepts and languages)</div>            
            <div>2. Operator Framework</div>            
            <div>3. Application Monitoring with Prometheus</div> 
            <div style="padding-left: 30px;">‚ñ∏ Deploy Prometheus and Grafana</div>
            <div style="padding-left: 30px;">‚ñ∏ Monitoring of a Spring Boot application</div>
            <p>Die Schulung wird von Christoph Raaflaub, unserem Software Architect & Middleware Engineer, durchgef√ºhrt. Christoph ist unser Spezialist in Sache automatisierte Software Delivery und teilt gerne seine langj√§hrige Erfahrung mit euch.</p>
            <p>Die OpenShift Dev Schulung wird bis max. 8 Teilnehmer durchgef√ºhrt. Auf Anfrage ist eine Durchf√ºhrung auch f√ºr ganze Gruppen m√∂glich, dediziert oder bei dir am Standort. Die Schulung ist in Deutsch oder Englisch m√∂glich.</p>
            <p>Ist das etwas f√ºr dich und deine Mitarbeiter? Gerne senden wir dir das Angebot zur OpenShift Dev Schulung zu und freuen uns auf deine Kontaktaufnahme.</p>
            <div><b>Ort:</b> Dediziert bei dir (oder an unseren Standorten in Bern/Z√ºrich)</div>
            <div><b>Kosten:</b> auf Anfrage</div>
            <div><b>Kontakt f√ºr R√ºckfragen:</b> <a href="mailto:hello@appuio.ch">hello@appuio.ch</a>, Anna Pfeifhofer</div>

- day: 23
  month: Mai
  title: "Was Arcades, gelbe Socken und Container gemeinsam haben"
  subtitle: |
            <img src="/images/blogposts/DevOps.jpg" alt="DevOps"/>
            Am 14. und 15. Mai 2019 fanden die DevOpsDays 2019 in Spreitenbach statt. APPUiO war als Platin-Sponsor und einem eigenen Stand vertreten. Aber was hat das alles nun mit Arcades, gelben Socken und Container zu tun? Erfahrt mehr in diesem Blogpost.
  details: |
            <p><em>Montag, 13. Mai, mittags</em><br />Der Bus (¬´Danke YOKKO¬ª) ist bereit, die Reise nach Spreitenbach in die Umweltarena anzutreten.  Das Ziel: Die DevOpsDays 2019. Bepackt mit lauter gelben Sachen trifft sich das APPUiO-Team in Z√ºrich. Nun heisst es: anpacken, aufbauen und schlussendlich...sich freuen.</p>
            <img src="/images/blogposts/DevOps1.JPG" alt="DevOps1"/>
            <p><em>Montag, 13. Mai, abends</em><br />¬´Yes it's done¬ª - Unser APPUiO-Stand steht bereit f√ºr die n√§chsten zwei Tage. Alle halfen mit viel Fleiss und Liebe zum Detail den Stand zu schm√ºcken und f√ºr die Teilnehmenden interessant zu gestalten. Das buntgemischte APPUiO-Team aus <a href="https://vshn.ch/">VSHNers</a> und <a href="https://www.puzzle.ch/de/">Puzzlers</a> gibt dem Stand wahrscheinlich das gewisse Etwas. Das Ergebnis l√§sst sich jedenfalls zeigen.</p>
            <img src="/images/blogposts/DevOps2.JPG" alt="DevOps2"/>
            <p><em>Dienstag, 14. Mai, morgens</em><br />Das Team wartet gespannt auf die G√§ste und verleiht dem Stand den letzten Schliff, damit die Teilnehmenden ein optimales APPUiO-Erlebnis geniessen k√∂nnen. Zwei Arcades stehen f√ºr die anstehenden Battles bereit. Im Gewinner-Pot liegen Vouchers von APPUiO und <a href="https://www.cloudscale.ch/de/">cloudscale.ch</a> und eine Lego Saturn V Rakete. Weiteres Spielvergn√ºgen bietet der Retro-Rohrfernseher mit der angeschlossenen Nintendo 64. Auf der gem√ºtlichen Sofaecke l√§sst es sich verweilen, sich austauschen oder die Pr√§sentationen verfolgen.</p>
            <p>Bereits vor den ersten Pr√§sentationen wagen sich einige Teilnehmenden in die H√∂hle der Arcade-L√∂wen und wollen den Highscore erreichen. Begeisterung zeigen die Besucher auch f√ºr unser <a href="https://appuio.ch/blog.html#APPUiO%20an%20den%20DevOpsDays%202018">APPUiOli</a>. In Gespr√§chen erfahren sie zudem mehr √ºber APPUiO, die n√§chsten Etappen und Ziele.</p>
            <img src="/images/blogposts/DevOps3.JPG" alt="DevOps3"/>
            <img src="/images/blogposts/DevOps4.JPG" alt="DevOps4"/>
            <img src="/images/blogposts/DevOps5.JPG" alt="DevOps5"/>
            <p><em>Dienstag, 14. Mai, abends</em><br />Der erste Tag ist geschafft! Bei einem Bier, einem super Ap√©ro und tollen Gespr√§chen l√§sst sich der Tag wunderbar ausklingen. Das Team durfte viele neue Kontakte kn√ºpfen und freut sich √ºber das grosse Interesse an APPUiO. Auch unsere Community war stolz vertreten, viele der Community-Mitglieder besuchte APPUiO am Stand. Dabei merken wir, wie gross die Community von APPUiO bereits geworden ist. Dies l√§sst auf weitere gemeinsame Innovationen und coole Events hoffen. Wir freuen uns, auf das, was kommt!</p>
            <img src="/images/blogposts/DevOps6.jpg" alt="DevOps6"/>
            <p><em>Mittwoch, 15. Mai, mittags</em><br />Neuer Tag, neues Gl√ºck? Das denken sich wahrscheinlich auch einige der Besucher und wagen erneut ein Arcade-Battle. F√ºr die APPUiO-Sockentr√§ger unter den Besuchern gibt es eine kleine √úberraschung: Als Merci f√ºr ihre Treue erhalten sie ein Mandelb√§rchen. ...und wer nicht S√ºsses mag, kann auch Saures haben‚Ä¶ Die Gummib√§rchen im APPUiO-Style helfen nicht nur bei den Spielbattles als Nervennahrung, sondern auch unserem <a href="https://vshn.ch/vshn/">Markus Speth</a>. Der APPUiO-Marketingexperte darf am Nachmittag in einem Pitch APPUiO vorstellen.</p>
            <img src="/images/blogposts/DevOps7.jpg" alt="DevOps7"/>
            <p><em>Mittwoch, 15. Mai, abends</em><br />Auch dieser Tag geht langsam zu Ende. Als Kr√∂nung werden die drei Gewinner des Arcade-Wettbewerbs - Alvise Dorigo, Michael Gerber und Carlo Speranza - geehrt. M√ºde aber zufrieden verabschiedet das APPUiO-Team die letzten Besucher. Nach der kurzen aber intensiven Aufr√§umaktion finden dann auch die ¬´APPUiOler¬ª den Weg nach Hause. Wir bedanken uns ganz herzlich f√ºr die tollen Bekanntschaften, spannenden Gespr√§chen und f√ºr euren Besuch an unserem Stand. Es hat uns einen Riesenspass gemacht! Und wir freuen uns, wenn es wieder heisst: DevOpsDays 2020!</p>
            <img src="/images/blogposts/DevOps8.JPG" alt="DevOps8"/>
            <img src="/images/blogposts/DevOps9.JPG" alt="DevOps9"/>
            <p>Und falls du Nervennahrung brauchst, kalte F√ºsse hast oder deinen Laptop in APPUiO-Style aufpimpen willst, dann meld dich bei uns.  #APPUiOGummib√§rchen #APPUiOSocken #Stickers ;-)</p>
            <p>Bis dann!</p>
            <img src="/images/blogposts/DevOps10.jpg" alt="DevOps10"/>

- day: 11
  month: Apr
  title: "Ein kurzer R√ºckblick auf das erste beerup von APPUiO"
  subtitle: |
            <img src="/images/blogposts/beerup.jpg" alt="Beerup"/>
            Am 28. M√§rz fand das erste beerup von APPUiO in der R√ºsterei in Z√ºrich statt. Dabei durften wir von den Inputs und Ideen unserer APPUiO Community profitieren und das Beisammensein bei einem Bier geniessen.
  details: |
            <p>Der Begriff beerup ist mit dem Ziel entstanden, die Community von APPUiO in ungezwungener Atmosph√§re zu vereinen und dabei √ºber die Bed√ºrfnisse der Community zum Thema Container und OpenShift zu sprechen. Was k√∂nnte APPUiO der Community bieten? Wie k√∂nnen wir noch mehr voneinander profitieren? Denn ihr wisst: Zusammen sind wir st√§rker, k√∂nnen Grosses in der IT-Welt erreichen und gemeinsam macht es einfach mehr Spass :-)</p>
            <p>Das erste beerup hat am 28. M√§rz im Anschluss an dem OpenShift RoundTable von Red Hat stattgefunden. Dabei durften wir rund 75 G√§ste in der R√ºsterei in Z√ºrich empfangen. Unsere Community konnte sich bei einem Ap√©ro verpflegen und sich √ºber dies und jenes austauschen. In den vielen tollen Gespr√§chen stiess das APPUiO-Team auf verschiedenste Ideen und Bed√ºrfnisse der Community. Durch das Feedback jedes Einzelnen durften wir eine grosse Anzahl an Inputs aufnehmen. Daf√ºr bedanken wir uns noch einmal herzlich bei euch und bei Red Hat f√ºr die Unterst√ºtzung.</p>
            <img src="/images/blogposts/RoundTable.JPG" alt="Roundtable"/>
            <img src="/images/blogposts/beerup1.jpg" alt="Beerup1"/>
            <br>
            <b>Doch wie geht es nun weiter?</b> 
            <div>Das APPUiO-Team wird sich einen √úberblick √ºber eure Ideen verschaffen und versuchen, sie baldm√∂glichst zu realisieren. Du kannst dich also jetzt schon √ºber die weitere Entwicklung von APPUiO freuen!</div>
            <b>Du hast noch einen Input oder eine Idee aber konntest diese nicht mitteilen?</b> 
            <div>Wir freuen uns jederzeit √ºber euer Feedback. <a href="mailto:hello@appuio.ch">Schreib uns</a> einfach und teile uns mit, wie wir APPUiO weiter verbessern k√∂nnen.</div>
            <p>Es hat Spass gemacht, eine so grosse Anzahl von Leuten aus den unterschiedlichsten Branchen und Kantonen bei uns am beerup begr√ºssen zu d√ºrfen, mit euch zu plaudern und euch n√§her kennenzulernen.</p>
            <p>Wir w√ºnschen euch eine gute Zeit und bis zum n√§chsten Mal!</p>
